#!/usr/bin/env python3
"""Volume projection collector for PostgreSQL, MSSQL, and Oracle."""

from __future__ import annotations

import argparse
import logging
import os
from datetime import datetime, timedelta, timezone
from pathlib import Path

from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

PRED_TABLES = {
    "collection_runs": "collection_runs",
    "table_size_snapshots": "table_size_snapshots",
    "growth_history": "growth_history",
    "database_snapshots": "database_snapshots",
}


def _load_env_file() -> None:
    for base in (Path.cwd(), Path(__file__).resolve().parents[4]):
        env_path = base / ".env"
        if env_path.exists():
            try:
                with open(env_path, encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith("#") and "=" in line:
                            k, _, v = line.partition("=")
                            k = k.strip()
                            v = v.strip().strip('"').strip("'")
                            if k and k not in os.environ:
                                os.environ[k] = v
            except Exception:
                pass
            break


def get_engine(database_url: str) -> Engine:
    connect_args = {}
    if database_url.startswith(("postgresql://", "postgresql+")):
        connect_args = {"connect_timeout": 10}
    return create_engine(database_url, pool_pre_ping=True, connect_args=connect_args, echo=False)


def _pred_table(dialect: str, name: str) -> str:
    if dialect == "oracle":
        return f"prediction_{PRED_TABLES[name]}"
    return f"prediction.{PRED_TABLES[name]}"


def _q_ident(dialect: str, name: str) -> str:
    if dialect == "mssql":
        return f"[{name}]"
    if dialect == "oracle":
        return name
    return f'"{name}"'


def _qualified_table(dialect: str, schema: str, table: str) -> str:
    if dialect == "mssql":
        return f"[{schema}].[{table}]"
    if dialect == "oracle":
        return f"{schema}.{table}"
    return f'"{schema}"."{table}"'


def _create_namespace(engine: Engine) -> None:
    d = engine.dialect.name
    with engine.connect() as conn:
        if d == "postgresql":
            conn.execute(text("CREATE SCHEMA IF NOT EXISTS prediction"))
        elif d == "mssql":
            conn.execute(text("IF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE name='prediction') EXEC('CREATE SCHEMA prediction')"))
        conn.commit()


def run_setup(engine: Engine) -> None:
    d = engine.dialect.name
    _create_namespace(engine)

    # Use prefixed table names in Oracle because creating a separate schema is not portable.
    cr = _pred_table(d, "collection_runs")
    ts = _pred_table(d, "table_size_snapshots")
    gh = _pred_table(d, "growth_history")
    ds = _pred_table(d, "database_snapshots")

    sqls = [
        f"""
        CREATE TABLE {cr} (
          run_id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
          started_at TIMESTAMP NOT NULL,
          completed_at TIMESTAMP,
          status VARCHAR(32) NOT NULL,
          tables_analyzed INTEGER
        )
        """,
        f"""
        CREATE TABLE {ts} (
          snapshot_id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
          run_id INTEGER NOT NULL,
          table_name VARCHAR(256) NOT NULL,
          schema_name VARCHAR(256) NOT NULL,
          snapshot_date DATE NOT NULL,
          row_count BIGINT NOT NULL,
          avg_row_size_bytes DOUBLE PRECISION,
          table_data_size_bytes BIGINT,
          index_size_bytes BIGINT,
          toast_size_bytes BIGINT,
          total_size_bytes BIGINT,
          bloat_ratio DOUBLE PRECISION,
          n_live_tup BIGINT,
          n_dead_tup BIGINT,
          n_tup_ins BIGINT,
          n_tup_upd BIGINT,
          n_tup_del BIGINT,
          n_tup_hot_upd BIGINT,
          stats_reset_at TIMESTAMP
        )
        """,
        f"""
        CREATE TABLE {gh} (
          growth_id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
          run_id INTEGER NOT NULL,
          table_name VARCHAR(256) NOT NULL,
          schema_name VARCHAR(256) NOT NULL,
          source_column VARCHAR(256) NOT NULL,
          period_start DATE NOT NULL,
          period_end DATE NOT NULL,
          rows_added BIGINT NOT NULL,
          cumulative_rows BIGINT NOT NULL
        )
        """,
        f"""
        CREATE TABLE {ds} (
          db_snapshot_id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
          run_id INTEGER NOT NULL,
          snapshot_date DATE NOT NULL,
          total_database_size_bytes BIGINT,
          shared_buffers VARCHAR(256),
          work_mem VARCHAR(256),
          temp_buffers VARCHAR(256),
          max_connections INTEGER,
          temp_files_count BIGINT,
          temp_bytes BIGINT
        )
        """,
    ]

    # Dialect-specific fallback for identity syntax.
    if d == "mssql":
        sqls = [s.replace("INTEGER GENERATED BY DEFAULT AS IDENTITY", "INT IDENTITY(1,1)") for s in sqls]
    elif d == "postgresql":
        sqls = [s.replace("INTEGER GENERATED BY DEFAULT AS IDENTITY", "SERIAL") for s in sqls]

    with engine.connect() as conn:
        for stmt in sqls:
            try:
                conn.execute(text(stmt))
            except Exception:
                # Table probably exists already.
                conn.rollback()
        conn.commit()

    logger.info("Volume projection setup complete for dialect=%s", d)


def _list_tables(conn, dialect: str, schema: str) -> list[str]:
    if dialect == "oracle":
        rows = conn.execute(
            text(
                """
                SELECT table_name
                FROM all_tables
                WHERE owner = UPPER(:schema)
                ORDER BY table_name
                """
            ),
            {"schema": schema},
        ).fetchall()
        return [r[0] for r in rows]

    rows = conn.execute(
        text(
            """
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = :schema
              AND table_type = 'BASE TABLE'
            ORDER BY table_name
            """
        ),
        {"schema": schema},
    ).fetchall()
    return [r[0] for r in rows]


def _insert_run(conn, dialect: str, started_at: datetime) -> int:
    cr = _pred_table(dialect, "collection_runs")
    conn.execute(
        text(f"INSERT INTO {cr} (started_at, status, tables_analyzed) VALUES (:started_at, 'running', 0)"),
        {"started_at": started_at},
    )
    # Fetch latest running row; adequate for this tooling workflow.
    row = conn.execute(
        text(f"SELECT run_id FROM {cr} ORDER BY run_id DESC"),
    ).fetchone()
    return int(row[0])


def _collect_table_snapshot(conn, dialect: str, run_id: int, schema: str, table_name: str, snapshot_date) -> None:
    ts = _pred_table(dialect, "table_size_snapshots")
    qualified = _qualified_table(dialect, schema, table_name)

    row_count = 0
    total_size = 0
    table_size = 0
    index_size = 0
    toast_size = 0
    avg_row_size = None
    bloat_ratio = None

    n_live_tup = n_dead_tup = n_tup_ins = n_tup_upd = n_tup_del = n_tup_hot_upd = 0
    stats_reset_at = None

    try:
        row_count = conn.execute(text(f"SELECT COUNT(*) FROM {qualified}")).scalar() or 0

        if dialect == "postgresql":
            size_row = conn.execute(
                text(
                    """
                    SELECT
                      pg_total_relation_size(:q) AS total_size,
                      pg_table_size(:q) AS table_size,
                      pg_indexes_size(:q) AS index_size
                    """
                ),
                {"q": qualified},
            ).fetchone()
            total_size = int(size_row[0] or 0)
            table_size = int(size_row[1] or 0)
            index_size = int(size_row[2] or 0)
            toast_size = max(total_size - table_size - index_size, 0)

            stat_row = conn.execute(
                text(
                    """
                    SELECT n_live_tup, n_dead_tup, n_tup_ins, n_tup_upd, n_tup_del, n_tup_hot_upd
                    FROM pg_stat_user_tables
                    WHERE schemaname = :schema AND relname = :table
                    """
                ),
                {"schema": schema, "table": table_name},
            ).fetchone()
            if stat_row:
                n_live_tup = int(stat_row[0] or 0)
                n_dead_tup = int(stat_row[1] or 0)
                n_tup_ins = int(stat_row[2] or 0)
                n_tup_upd = int(stat_row[3] or 0)
                n_tup_del = int(stat_row[4] or 0)
                n_tup_hot_upd = int(stat_row[5] or 0)

        elif dialect == "mssql":
            size_row = conn.execute(
                text(
                    """
                    SELECT
                      SUM(ps.used_page_count) * 8 * 1024 AS total_size,
                      SUM(ps.in_row_data_page_count + ps.lob_used_page_count + ps.row_overflow_used_page_count) * 8 * 1024 AS table_size,
                      SUM(ps.used_page_count - (ps.in_row_data_page_count + ps.lob_used_page_count + ps.row_overflow_used_page_count)) * 8 * 1024 AS index_size
                    FROM sys.dm_db_partition_stats ps
                    JOIN sys.tables t ON ps.object_id = t.object_id
                    JOIN sys.schemas s ON t.schema_id = s.schema_id
                    WHERE s.name = :schema AND t.name = :table
                    """
                ),
                {"schema": schema, "table": table_name},
            ).fetchone()
            total_size = int(size_row[0] or 0) if size_row else 0
            table_size = int(size_row[1] or 0) if size_row else 0
            index_size = int(size_row[2] or 0) if size_row else 0
            toast_size = 0

        elif dialect == "oracle":
            total_size = int(
                conn.execute(
                    text(
                        """
                        SELECT NVL(SUM(bytes),0)
                        FROM all_segments
                        WHERE owner = UPPER(:schema)
                          AND segment_name = UPPER(:table)
                        """
                    ),
                    {"schema": schema, "table": table_name},
                ).scalar()
                or 0
            )
            table_size = total_size
            index_size = int(
                conn.execute(
                    text(
                        """
                        SELECT NVL(SUM(s.bytes),0)
                        FROM all_segments s
                        JOIN all_indexes i
                          ON i.owner = s.owner
                         AND i.index_name = s.segment_name
                        WHERE i.table_owner = UPPER(:schema)
                          AND i.table_name = UPPER(:table)
                        """
                    ),
                    {"schema": schema, "table": table_name},
                ).scalar()
                or 0
            )
            total_size = max(total_size, table_size + index_size)

        if row_count > 0 and table_size > 0:
            avg_row_size = round(float(table_size) / float(row_count), 2)

        conn.execute(
            text(
                f"""
                INSERT INTO {ts} (
                  run_id, table_name, schema_name, snapshot_date,
                  row_count, avg_row_size_bytes,
                  table_data_size_bytes, index_size_bytes, toast_size_bytes, total_size_bytes,
                  bloat_ratio, n_live_tup, n_dead_tup, n_tup_ins, n_tup_upd, n_tup_del, n_tup_hot_upd,
                  stats_reset_at
                ) VALUES (
                  :run_id, :table_name, :schema_name, :snapshot_date,
                  :row_count, :avg_row_size_bytes,
                  :table_data_size_bytes, :index_size_bytes, :toast_size_bytes, :total_size_bytes,
                  :bloat_ratio, :n_live_tup, :n_dead_tup, :n_tup_ins, :n_tup_upd, :n_tup_del, :n_tup_hot_upd,
                  :stats_reset_at
                )
                """
            ),
            {
                "run_id": run_id,
                "table_name": table_name,
                "schema_name": schema,
                "snapshot_date": snapshot_date,
                "row_count": row_count,
                "avg_row_size_bytes": avg_row_size,
                "table_data_size_bytes": table_size,
                "index_size_bytes": index_size,
                "toast_size_bytes": toast_size,
                "total_size_bytes": total_size,
                "bloat_ratio": bloat_ratio,
                "n_live_tup": n_live_tup,
                "n_dead_tup": n_dead_tup,
                "n_tup_ins": n_tup_ins,
                "n_tup_upd": n_tup_upd,
                "n_tup_del": n_tup_del,
                "n_tup_hot_upd": n_tup_hot_upd,
                "stats_reset_at": stats_reset_at,
            },
        )
    except Exception as e:
        logger.warning("Snapshot collection failed for %s.%s: %s", schema, table_name, e)
        conn.rollback()


def _collect_growth_history(conn, dialect: str, run_id: int, schema: str, table_name: str, cutoff_date) -> None:
    gh = _pred_table(dialect, "growth_history")

    try:
        if dialect == "oracle":
            col_row = conn.execute(
                text(
                    """
                    SELECT column_name
                    FROM all_tab_columns
                    WHERE owner = UPPER(:schema)
                      AND table_name = UPPER(:table)
                      AND LOWER(column_name) IN ('created_at', 'created_date', 'inserted_at')
                    ORDER BY CASE LOWER(column_name)
                      WHEN 'created_at' THEN 1
                      WHEN 'created_date' THEN 2
                      ELSE 3
                    END
                    """
                ),
                {"schema": schema, "table": table_name},
            ).fetchone()
        else:
            col_row = conn.execute(
                text(
                    """
                    SELECT column_name
                    FROM information_schema.columns
                    WHERE table_schema = :schema
                      AND table_name = :table
                      AND column_name IN ('created_at', 'created_date', 'inserted_at')
                    ORDER BY CASE column_name
                      WHEN 'created_at' THEN 1
                      WHEN 'created_date' THEN 2
                      ELSE 3
                    END
                    """
                ),
                {"schema": schema, "table": table_name},
            ).fetchone()

        if not col_row:
            return

        source_column = col_row[0]
        qualified = _qualified_table(dialect, schema, table_name)
        col = _q_ident(dialect, source_column)

        if dialect == "postgresql":
            q = text(
                f"""
                SELECT date_trunc('month', {col})::date AS period_start, COUNT(*) AS rows_added
                FROM {qualified}
                WHERE {col} >= :cutoff
                GROUP BY date_trunc('month', {col})
                ORDER BY period_start
                """
            )
        elif dialect == "mssql":
            q = text(
                f"""
                SELECT DATEFROMPARTS(YEAR({col}), MONTH({col}), 1) AS period_start, COUNT(*) AS rows_added
                FROM {qualified}
                WHERE {col} >= :cutoff
                GROUP BY DATEFROMPARTS(YEAR({col}), MONTH({col}), 1)
                ORDER BY period_start
                """
            )
        else:
            q = text(
                f"""
                SELECT TRUNC({col}, 'MM') AS period_start, COUNT(*) AS rows_added
                FROM {qualified}
                WHERE {col} >= :cutoff
                GROUP BY TRUNC({col}, 'MM')
                ORDER BY period_start
                """
            )

        rows = conn.execute(q, {"cutoff": cutoff_date}).fetchall()
        if not rows:
            return

        cumulative = 0
        for period_start, rows_added in rows:
            cumulative += int(rows_added or 0)
            period_end = period_start + timedelta(days=32)
            conn.execute(
                text(
                    f"""
                    INSERT INTO {gh} (
                      run_id, table_name, schema_name, source_column,
                      period_start, period_end, rows_added, cumulative_rows
                    ) VALUES (
                      :run_id, :table_name, :schema_name, :source_column,
                      :period_start, :period_end, :rows_added, :cumulative_rows
                    )
                    """
                ),
                {
                    "run_id": run_id,
                    "table_name": table_name,
                    "schema_name": schema,
                    "source_column": source_column,
                    "period_start": period_start,
                    "period_end": period_end,
                    "rows_added": int(rows_added or 0),
                    "cumulative_rows": cumulative,
                },
            )
    except Exception as e:
        logger.warning("Growth history collection failed for %s.%s: %s", schema, table_name, e)
        conn.rollback()


def _collect_database_snapshot(conn, dialect: str, run_id: int, snapshot_date) -> None:
    ds = _pred_table(dialect, "database_snapshots")

    total_size = 0
    shared_buffers = None
    work_mem = None
    temp_buffers = None
    max_connections = None
    temp_files_count = None
    temp_bytes = None

    try:
        if dialect == "postgresql":
            total_size = int(conn.execute(text("SELECT pg_database_size(current_database())")).scalar() or 0)
            cfg = conn.execute(
                text(
                    """
                    SELECT name, setting
                    FROM pg_settings
                    WHERE name IN ('shared_buffers', 'work_mem', 'temp_buffers', 'max_connections')
                    """
                )
            ).fetchall()
            cfgm = {r[0]: r[1] for r in cfg}
            shared_buffers = cfgm.get("shared_buffers")
            work_mem = cfgm.get("work_mem")
            temp_buffers = cfgm.get("temp_buffers")
            max_connections = int(cfgm.get("max_connections") or 0)
            trow = conn.execute(text("SELECT temp_files, temp_bytes FROM pg_stat_database WHERE datname=current_database()"))
            trow = trow.fetchone()
            if trow:
                temp_files_count = int(trow[0] or 0)
                temp_bytes = int(trow[1] or 0)

        elif dialect == "mssql":
            total_size = int(conn.execute(text("SELECT SUM(size) * 8 * 1024 FROM sys.database_files")).scalar() or 0)
            cfg = conn.execute(
                text(
                    """
                    SELECT name, value_in_use
                    FROM sys.configurations
                    WHERE name IN ('max server memory (MB)', 'user connections')
                    """
                )
            ).fetchall()
            cfgm = {r[0]: r[1] for r in cfg}
            shared_buffers = str(cfgm.get("max server memory (MB)")) if cfgm.get("max server memory (MB)") is not None else None
            max_connections = int(cfgm.get("user connections") or 0) if cfgm.get("user connections") is not None else None

        elif dialect == "oracle":
            total_size = int(conn.execute(text("SELECT NVL(SUM(bytes),0) FROM user_segments")).scalar() or 0)

        conn.execute(
            text(
                f"""
                INSERT INTO {ds} (
                  run_id, snapshot_date, total_database_size_bytes,
                  shared_buffers, work_mem, temp_buffers, max_connections,
                  temp_files_count, temp_bytes
                ) VALUES (
                  :run_id, :snapshot_date, :total_database_size_bytes,
                  :shared_buffers, :work_mem, :temp_buffers, :max_connections,
                  :temp_files_count, :temp_bytes
                )
                """
            ),
            {
                "run_id": run_id,
                "snapshot_date": snapshot_date,
                "total_database_size_bytes": total_size,
                "shared_buffers": shared_buffers,
                "work_mem": work_mem,
                "temp_buffers": temp_buffers,
                "max_connections": max_connections,
                "temp_files_count": temp_files_count,
                "temp_bytes": temp_bytes,
            },
        )
    except Exception as e:
        logger.warning("Database snapshot collection failed: %s", e)
        conn.rollback()


def run_collect(engine: Engine, schema: str) -> None:
    dialect = engine.dialect.name
    started_at = datetime.now(timezone.utc)
    snapshot_date = started_at.date()

    with engine.connect() as conn:
        run_id = _insert_run(conn, dialect, started_at)
        conn.commit()

        logger.info("Started volume collection run_id=%s dialect=%s", run_id, dialect)

        try:
            tables = _list_tables(conn, dialect, schema)
            for t in tables:
                _collect_table_snapshot(conn, dialect, run_id, schema, t, snapshot_date)

            cutoff = snapshot_date - timedelta(days=730)
            for t in tables:
                _collect_growth_history(conn, dialect, run_id, schema, t, cutoff)

            _collect_database_snapshot(conn, dialect, run_id, snapshot_date)

            cr = _pred_table(dialect, "collection_runs")
            conn.execute(
                text(
                    f"""
                    UPDATE {cr}
                    SET completed_at = :completed_at,
                        status = 'success',
                        tables_analyzed = :tables_analyzed
                    WHERE run_id = :run_id
                    """
                ),
                {
                    "completed_at": datetime.now(timezone.utc),
                    "tables_analyzed": len(tables),
                    "run_id": run_id,
                },
            )
            conn.commit()
            logger.info("Collection complete. tables_analyzed=%s", len(tables))

        except Exception:
            cr = _pred_table(dialect, "collection_runs")
            conn.execute(
                text(f"UPDATE {cr} SET completed_at = :completed_at, status = 'failed' WHERE run_id = :run_id"),
                {"completed_at": datetime.now(timezone.utc), "run_id": run_id},
            )
            conn.commit()
            raise


def main() -> None:
    _load_env_file()
    parser = argparse.ArgumentParser(description="Volume projection collector")
    parser.add_argument("database_url", nargs="?", default=os.environ.get("DATABASE_URL"))
    parser.add_argument("--setup", action="store_true", help="Create prediction tables")
    parser.add_argument("--collect", action="store_true", help="Collect volume metrics")
    parser.add_argument("--schema", default=os.environ.get("DATABASE_SCHEMA") or os.environ.get("SCHEMA") or "public")
    args = parser.parse_args()

    if not args.database_url:
        parser.error("database_url required (argument or DATABASE_URL env)")
    if not args.setup and not args.collect:
        parser.error("Specify --setup and/or --collect")

    engine = get_engine(args.database_url)
    if args.setup:
        run_setup(engine)
    if args.collect:
        run_collect(engine, schema=args.schema)


if __name__ == "__main__":
    main()
